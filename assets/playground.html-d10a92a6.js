const e=JSON.parse(`{"key":"v-4e2a7469","path":"/posts/playground.html","title":"Creando un pipeline de datos: let's get dirty","lang":"es-ES","frontmatter":{"title":"Creando un pipeline de datos: let's get dirty","cover":"/assets/images/pipeline.jpeg","icon":"file","order":3,"author":"Leandro Gutierrez","date":"2024-04-01T00:00:00.000Z","category":["Bases de Datos"],"tag":["Bases de Datos","Ingenieria de datos","Docker","MySQL","Kafka"],"sticky":true,"star":true,"disableCopy":true,"description":"En este post vamos a ver una de las opciones de sincronización mas utilizadas de momento, haremos uso del CDC de nuestra base de datos transaccional MySQL para publicar cada una de sus entradas en nuestro stack de Kafka.","head":[["meta",{"property":"og:url","content":"https://leandrogutierrez148.github.io/posts/playground.html"}],["meta",{"property":"og:title","content":"Creando un pipeline de datos: let's get dirty"}],["meta",{"property":"og:description","content":"En este post vamos a ver una de las opciones de sincronización mas utilizadas de momento, haremos uso del CDC de nuestra base de datos transaccional MySQL para publicar cada una de sus entradas en nuestro stack de Kafka."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://leandrogutierrez148.github.io/assets/images/pipeline.jpeg"}],["meta",{"property":"og:locale","content":"es-ES"}],["meta",{"property":"og:updated_time","content":"2024-04-08T12:06:25.000Z"}],["meta",{"name":"twitter:card","content":"summary_large_image"}],["meta",{"name":"twitter:image:alt","content":"Creando un pipeline de datos: let's get dirty"}],["meta",{"property":"article:author","content":"Leandro Gutierrez"}],["meta",{"property":"article:tag","content":"Bases de Datos"}],["meta",{"property":"article:tag","content":"Ingenieria de datos"}],["meta",{"property":"article:tag","content":"Docker"}],["meta",{"property":"article:tag","content":"MySQL"}],["meta",{"property":"article:tag","content":"Kafka"}],["meta",{"property":"article:published_time","content":"2024-04-01T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2024-04-08T12:06:25.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Creando un pipeline de datos: let's get dirty\\",\\"image\\":[\\"https://leandrogutierrez148.github.io/assets/images/pipeline.jpeg\\"],\\"datePublished\\":\\"2024-04-01T00:00:00.000Z\\",\\"dateModified\\":\\"2024-04-08T12:06:25.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Leandro Gutierrez\\"}]}"]]},"headers":[{"level":2,"title":"Introduccion","slug":"introduccion","link":"#introduccion","children":[{"level":3,"title":"[WIP]","slug":"wip","link":"#wip","children":[]}]}],"git":{"createdTime":1712573928000,"updatedTime":1712577985000,"contributors":[{"name":"Leandro Gutierrez","email":"lgutierrez@onebittech.com","commits":3}]},"readingTime":{"minutes":1.09,"words":328},"filePathRelative":"posts/playground.md","localizedDate":"1 de abril de 2024","excerpt":"<p>En este post vamos a ver una de las opciones de sincronización mas utilizadas de momento, haremos uso del CDC de nuestra base de datos transaccional MySQL para publicar cada una de sus entradas en nuestro stack de Kafka.</p>\\n","copyright":{"author":"Leandro Gutierrez"},"autoDesc":true}`);export{e as data};
